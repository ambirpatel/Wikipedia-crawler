{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia crawler which collects all the links from given page and also check connection between all of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time,datetime,random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "urllib is used to make request to open website we want. bs4 is package which will parse the html page and store it so we can access and manipulate it. networkx is package for creating graph from given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/'\n",
    "html = urlopen(url, timeout=(6.05))\n",
    "bs = soup(html, 'html.parser')\n",
    "html.close()\n",
    "bs.find(\"div\",{ \"class\":\"navbox\"}).decompose()\n",
    "bs.find(\"div\",{\"class\":\"navbox authority-control\"}).decompose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made request to wikipedia page and stored that page using bs4. Since we do not want links from navigation box in wikipedia page we just deleted those divisions using decompose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages, title = [], []\n",
    "edge_list = pd.DataFrame(columns=['V1','V2'])\n",
    "for link in bs.find(\"div\", {\"id\":\"bodyContent\"}).findAll(\"a\",\n",
    "                   href=re.compile(\"^(/wiki/)((?!:)(?!disambiguation)(?!International_Standard_Book_Number).)*$\")):\n",
    "    if link.attrs['title'] not in title:\n",
    "        pages.append(link.attrs['href'])\n",
    "        title.append(link.attrs['title'])\n",
    "\n",
    "M_art = pd.DataFrame(zip(title,pages),columns=['Article','Link'])\n",
    "edge_list['V2'], edge_list['V1'] = title, bs.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are checking every link on given wikipedia article and storing it only if it points to another wikipedia article. We are ignoring some unneccessary links such as ISBN, disambiguation. Our code does not store duplicate links or articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are storing titles of all the articles and making edgelist out of them. Means if one article points to another article there is edge between them(directed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pages:\n",
    "    url = 'https://en.wikipedia.org'+i    \n",
    "    time.sleep(random.randint(2,5))\n",
    "    random.seed(datetime.datetime.now())\n",
    "    html = urlopen(url, timeout=(6.05))\n",
    "    bs = soup(html, 'html.parser')\n",
    "    html.close()\n",
    "    sub_title, sub_pages = [], []\n",
    "    sub_EL = pd.DataFrame(columns=['V1','V2'])\n",
    "    for link in bs.find(\"div\", {\"id\":\"bodyContent\"}).findAll(\"a\",\n",
    "                       href=re.compile(\"^(/wiki/)((?!:)(?!disambiguation).)*$\")):\t\n",
    "        if link.attrs['href'] not in sub_pages:\n",
    "            if link.attrs['title'] in title:\n",
    "                sub_title.append(link.attrs['title'])\n",
    "                sub_pages.append(link.attrs['href'])\n",
    "                \n",
    "    sub_EL['V2'] = sub_title\n",
    "    sub_EL['V1'] = bs.title.text.replace(' - Wikipedia', '')\n",
    "    M_art_sub = pd.DataFrame(zip(sub_title,sub_pages), columns=['Article','Link'])\n",
    "    M_art = M_art.append(M_art_sub,  ignore_index = True)\n",
    "    edge_list = edge_list.append(sub_EL, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have some links from the page of our choice. Now we have to check connection between all those pages, hence we are going on each of those pages and checking if its pointing to one of its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will againg get more edges from above which we will append to our original edge list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "for i in range(len(edge_list)):\n",
    "     G.add_edge(edge_list.iloc[i,0],edge_list.iloc[i,1])\n",
    "plt.figure(figsize=(8,5))\n",
    "nx.draw(G, node_size=20, node_color=\"blue\",  alpha=0.5,with_labels = False)\n",
    "plt.savefig(\"______.svg\", dpi=100)\n",
    "nx.write_gml(G,'______.gml.gz')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a edgelist from which we can construct graph for visulization and calculate its statistical network properties such as degree distribution, clustering coefficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
